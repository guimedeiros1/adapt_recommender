{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 78 | Number of movies = 405\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/rating_explicit_only.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "n_users = df.learner_id.unique().shape[0]\n",
    "n_items = df.movie_id.unique().shape[0]\n",
    "\n",
    "print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum learner_id in this list is= 111\n"
     ]
    }
   ],
   "source": [
    "id_max = df.sort_values(by=['learner_id'], ascending=False).head(n=1)\n",
    "\n",
    "id_max = int(id_max.learner_id)\n",
    "\n",
    "print('The maximum learner_id in this list is= ' + str(id_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = pd.read_csv('../data/movie.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "df['movie_knowledge_area'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn = pd.read_csv('../data/learner.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "lrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#copy the value of knowledge area from movies.csv\n",
    "for index, row in df.iterrows():\n",
    "    target_mv = mv.loc[mv.id == row.movie_id]\n",
    "    df.movie_knowledge_area.iloc[index] = target_mv.movie_knowledge_area.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_knowledge_area\n",
       "BL    3.457831\n",
       "EN    3.674419\n",
       "HT    3.426752\n",
       "LT    3.282427\n",
       "MT    3.600000\n",
       "PH    3.525510\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rating mean grouped by movie knowledge area\n",
    "\n",
    "mean_ratings = df.groupby(['movie_knowledge_area'])['rating'].mean()\n",
    "\n",
    "mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2426, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the profile features to the ratings dataframe\n",
    "\n",
    "df['level_of_education'], df['level_of_english'], df['level_of_literature'], df['level_of_history'], df['level_of_biology'], df['level_of_physics'], df['level_of_math'], df['learning_goal'], df['learning_style'] = ['', '', '', '', '', '', '', '', ''] \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#copy the profile levels from learner.csv\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    target_lrn = lrn.loc[lrn.id == row.learner_id]\n",
    "    df.level_of_education.iloc[index] = target_lrn.level_of_education.values[0]\n",
    "    df.level_of_english.iloc[index] = target_lrn.level_of_english.values[0]\n",
    "    df.level_of_literature.iloc[index] = target_lrn.level_of_literature.values[0]\n",
    "    df.level_of_history.iloc[index] = target_lrn.level_of_history.values[0]\n",
    "    df.level_of_biology.iloc[index] = target_lrn.level_of_biology.values[0]\n",
    "    df.level_of_physics.iloc[index] = target_lrn.level_of_physics.values[0]\n",
    "    df.level_of_math.iloc[index] = target_lrn.level_of_math.values[0]\n",
    "    df.learning_goal.iloc[index] = target_lrn.learning_goal.values[0]\n",
    "    df.learning_style.iloc[index] = target_lrn.learning_style.values[0]\n",
    "#     print(target_lrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rating mean grouped by levels\n",
    "\n",
    "mean_education = df.groupby(['level_of_education'])['rating'].mean()\n",
    "mean_english = df.groupby(['level_of_english'])['rating'].mean()\n",
    "mean_literature = df.groupby(['level_of_literature'])['rating'].mean()\n",
    "mean_history = df.groupby(['level_of_history'])['rating'].mean()\n",
    "mean_biology = df.groupby(['level_of_biology'])['rating'].mean()\n",
    "mean_physics = df.groupby(['level_of_physics'])['rating'].mean()\n",
    "mean_math = df.groupby(['level_of_math'])['rating'].mean()\n",
    "mean_lgoal = df.groupby(['learning_goal'])['rating'].mean()\n",
    "mean_lstyle = df.groupby(['learning_style'])['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "level_of_education\n",
      "DT    3.135674\n",
      "EF    5.000000\n",
      "EM    3.790541\n",
      "ES    3.008292\n",
      "MT    3.394343\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_english\n",
      "HL    3.198221\n",
      "LL    3.910000\n",
      "ML    3.184840\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_literature\n",
      "HL    3.456967\n",
      "LL    2.880282\n",
      "ML    3.282482\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_history\n",
      "HL    3.702875\n",
      "LL    3.261628\n",
      "ML    3.117095\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_biology\n",
      "HL    3.940120\n",
      "LL    3.231368\n",
      "ML    3.130147\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_physics\n",
      "HL    3.236318\n",
      "LL    3.351049\n",
      "ML    3.149733\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_math\n",
      "HL    3.002795\n",
      "LL    3.743976\n",
      "ML    3.438914\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "learning_goal\n",
      "LLL    3.186104\n",
      "STL    3.260477\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "learning_style\n",
      "GLB    3.329248\n",
      "SQN    3.115641\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Means:\\n' + str(mean_education) + '\\n\\n' + str(mean_english) + '\\n\\n' + str(mean_literature) + '\\n\\n' + str(mean_history) + '\\n\\n' + str(mean_biology) + '\\n\\n' + str(mean_physics) + '\\n\\n' + str(mean_math)+ '\\n\\n' + str(mean_lgoal)+ '\\n\\n' + str(mean_lstyle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/ratings_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6666666666666665"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the item's rating mean\n",
    "\n",
    "# df.rating = pd.to_numeric(df.rating, downcast = 'integer', errors='coerce')\n",
    "\n",
    "items_mean = df.groupby(['movie_id'])['rating'].mean()\n",
    "overall_mean = items_mean.mean()\n",
    "\n",
    "items_mean[195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.3642514867149e-16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the item bias\n",
    "\n",
    "items_bias = items_mean - overall_mean\n",
    "\n",
    "overall_bias = item_bias.mean()\n",
    "\n",
    "overall_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #get specific user bias (baseline)\n",
    "\n",
    "# def user_bias(user):\n",
    "#     u_index = users_mean.index.get_loc(user) #get integer index through the label index\n",
    "#     u_mean = users_mean[u_index] #get user mean\n",
    "#     u_bias = u_mean - overall_mean #compute user bias\n",
    "#     return(u_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# save the bias on dataframe\n",
    "df['profile_bias'] = np.nan\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row.level_of_english == 'HL':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_english[0])\n",
    "    elif row.level_of_english == 'LL':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_english[1])\n",
    "    elif row.level_of_english == 'ML':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_english[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          id  rating  predicted_rating                   context_time  \\\n",
       "0         1       5               NaN  2017-11-27 18:02:03.302679+00   \n",
       "1         2       3               NaN  2017-11-27 18:02:03.476649+00   \n",
       "2         4       5               NaN  2017-11-27 18:02:03.591744+00   \n",
       "3         5       4               NaN  2017-11-27 18:02:03.643241+00   \n",
       "4         3       2               NaN   2017-11-27 19:35:26.62297+00   \n",
       "5     12375       2               NaN  2017-12-07 17:06:29.960877+00   \n",
       "6     12376       4               NaN  2017-12-07 17:06:29.990888+00   \n",
       "7     12377       1               NaN  2017-12-07 17:06:30.028038+00   \n",
       "8     12378       5               NaN  2017-12-07 17:06:30.049265+00   \n",
       "9     12379       3               NaN  2017-12-07 17:06:30.063373+00   \n",
       "10    12380       1               NaN  2017-12-07 17:06:30.081157+00   \n",
       "11    12381       2               NaN  2017-12-07 17:06:30.092964+00   \n",
       "12    12382       4               NaN  2017-12-07 17:06:30.125262+00   \n",
       "13    12383       3               NaN  2017-12-07 17:06:30.161178+00   \n",
       "14    12384       5               NaN  2017-12-07 17:06:30.189218+00   \n",
       "15    12385       5               NaN  2017-12-07 17:06:30.220728+00   \n",
       "16    12386       4               NaN  2017-12-07 17:06:30.245876+00   \n",
       "17    12387       2               NaN  2017-12-07 17:06:30.269306+00   \n",
       "18    12388       4               NaN  2017-12-07 17:06:30.328014+00   \n",
       "19    12389       4               NaN  2017-12-07 17:06:30.388435+00   \n",
       "20    12390       2               NaN  2017-12-07 17:06:30.445691+00   \n",
       "21    12391       3               NaN  2017-12-07 17:06:30.478007+00   \n",
       "22    12392       3               NaN  2017-12-07 17:06:30.514988+00   \n",
       "23    12393       3               NaN  2017-12-07 17:06:30.561526+00   \n",
       "24    12394       1               NaN  2017-12-07 17:06:30.596225+00   \n",
       "25     2031       4               NaN  2017-12-04 17:44:16.135757+00   \n",
       "26     2032       5               NaN  2017-12-04 17:44:16.149966+00   \n",
       "27     8511       2               NaN  2017-12-07 12:28:33.495773+00   \n",
       "28     2196       5         17.120539  2017-12-04 17:48:02.099639+00   \n",
       "29     2108       4         17.106332   2017-12-04 17:48:02.10834+00   \n",
       "...     ...     ...               ...                            ...   \n",
       "2396  34073       4               NaN  2017-12-16 16:20:48.777509+00   \n",
       "2397  34092       3               NaN  2017-12-16 16:20:48.783502+00   \n",
       "2398  34093       4               NaN  2017-12-16 16:20:48.788893+00   \n",
       "2399  34068       1               NaN  2017-12-16 16:20:48.795471+00   \n",
       "2400  34069       3               NaN  2017-12-16 16:20:48.803023+00   \n",
       "2401  34128       5               NaN  2017-12-18 13:01:12.513444+00   \n",
       "2402  34097       3               NaN  2017-12-16 16:20:48.608488+00   \n",
       "2403  34096       3               NaN  2017-12-16 16:20:48.614539+00   \n",
       "2404  34095       4               NaN  2017-12-16 16:20:48.620003+00   \n",
       "2405  34094       4               NaN  2017-12-16 16:20:48.625899+00   \n",
       "2406  34075       3               NaN  2017-12-16 16:20:48.633235+00   \n",
       "2407  34827       4               NaN  2017-12-27 15:58:51.539504+00   \n",
       "2408  34828       3               NaN  2017-12-27 15:58:51.578317+00   \n",
       "2409  34829       3               NaN  2017-12-27 15:58:51.618349+00   \n",
       "2410  34830       5               NaN    2017-12-27 15:58:51.6663+00   \n",
       "2411  34831       3               NaN  2017-12-27 15:58:51.690948+00   \n",
       "2412  34832       3               NaN  2017-12-27 15:58:51.743596+00   \n",
       "2413  34833       5               NaN   2017-12-27 15:58:51.78189+00   \n",
       "2414  34834       3               NaN  2017-12-27 15:58:51.836594+00   \n",
       "2415  34835       4               NaN  2017-12-27 15:58:51.913171+00   \n",
       "2416  34836       3               NaN  2017-12-27 15:58:52.041963+00   \n",
       "2417  34837       3               NaN  2017-12-27 15:58:52.078261+00   \n",
       "2418  34838       5               NaN  2017-12-27 15:58:52.106286+00   \n",
       "2419  34839       3               NaN  2017-12-27 15:58:52.130723+00   \n",
       "2420  34840       4               NaN  2017-12-27 15:58:52.147414+00   \n",
       "2421  34841       4               NaN  2017-12-27 15:58:52.215562+00   \n",
       "2422  34842       3               NaN  2017-12-27 15:58:52.255561+00   \n",
       "2423  34843       5               NaN  2017-12-27 15:58:52.299132+00   \n",
       "2424  34844       3               NaN  2017-12-27 15:58:52.333936+00   \n",
       "2425  34845       5               NaN  2017-12-27 15:58:52.353326+00   \n",
       "\n",
       "      movie_id  learner_id movie_knowledge_area level_of_education  \\\n",
       "0          195           1                  NaN                 DT   \n",
       "1            4           1                   HT                 DT   \n",
       "2          175           1                   LT                 DT   \n",
       "3          241           1                  NaN                 DT   \n",
       "4           85           1                   BL                 DT   \n",
       "5          228          37                   LT                 ES   \n",
       "6          264          37                   BL                 ES   \n",
       "7          403          37                   PH                 ES   \n",
       "8          280          37                   PH                 ES   \n",
       "9          250          37                  NaN                 ES   \n",
       "10         102          37                   LT                 ES   \n",
       "11          96          37                  NaN                 ES   \n",
       "12         224          37                   BL                 ES   \n",
       "13         251          37                  NaN                 ES   \n",
       "14          71          37                   EN                 ES   \n",
       "15         294          37                   LT                 ES   \n",
       "16         241          37                  NaN                 ES   \n",
       "17         170          37                  NaN                 ES   \n",
       "18         377          37                  NaN                 ES   \n",
       "19         244          37                  NaN                 ES   \n",
       "20          11          37                   LT                 ES   \n",
       "21          78          37                  NaN                 ES   \n",
       "22          68          37                  NaN                 ES   \n",
       "23         185          37                   HT                 ES   \n",
       "24         345          37                   HT                 ES   \n",
       "25          23           7                   BL                 MT   \n",
       "26         211           7                   HT                 MT   \n",
       "27         184          29                  NaN                 EM   \n",
       "28         165           7                   HT                 MT   \n",
       "29          77           7                   LT                 MT   \n",
       "...        ...         ...                  ...                ...   \n",
       "2396       349         107                   HT                 MT   \n",
       "2397       368         107                   HT                 MT   \n",
       "2398       369         107                   HT                 MT   \n",
       "2399       344         107                  NaN                 MT   \n",
       "2400       345         107                   HT                 MT   \n",
       "2401        65         108                   PH                 ES   \n",
       "2402       373         107                  NaN                 MT   \n",
       "2403       372         107                  NaN                 MT   \n",
       "2404       371         107                   PH                 MT   \n",
       "2405       370         107                  NaN                 MT   \n",
       "2406       351         107                   MT                 MT   \n",
       "2407        66         111                  NaN                 DT   \n",
       "2408       368         111                   HT                 DT   \n",
       "2409       132         111                  NaN                 DT   \n",
       "2410       191         111                   HT                 DT   \n",
       "2411       123         111                   HT                 DT   \n",
       "2412       165         111                   HT                 DT   \n",
       "2413       288         111                  NaN                 DT   \n",
       "2414        97         111                   MT                 DT   \n",
       "2415       257         111                  NaN                 DT   \n",
       "2416        35         111                  NaN                 DT   \n",
       "2417       167         111                  NaN                 DT   \n",
       "2418        17         111                   HT                 DT   \n",
       "2419       262         111                   BL                 DT   \n",
       "2420       138         111                   BL                 DT   \n",
       "2421       326         111                  NaN                 DT   \n",
       "2422       279         111                  NaN                 DT   \n",
       "2423       229         111                   PH                 DT   \n",
       "2424       187         111                  NaN                 DT   \n",
       "2425       334         111                  NaN                 DT   \n",
       "\n",
       "     level_of_english level_of_literature level_of_history level_of_biology  \\\n",
       "0                  HL                  ML               LL               LL   \n",
       "1                  HL                  ML               LL               LL   \n",
       "2                  HL                  ML               LL               LL   \n",
       "3                  HL                  ML               LL               LL   \n",
       "4                  HL                  ML               LL               LL   \n",
       "5                  ML                  LL               LL               ML   \n",
       "6                  ML                  LL               LL               ML   \n",
       "7                  ML                  LL               LL               ML   \n",
       "8                  ML                  LL               LL               ML   \n",
       "9                  ML                  LL               LL               ML   \n",
       "10                 ML                  LL               LL               ML   \n",
       "11                 ML                  LL               LL               ML   \n",
       "12                 ML                  LL               LL               ML   \n",
       "13                 ML                  LL               LL               ML   \n",
       "14                 ML                  LL               LL               ML   \n",
       "15                 ML                  LL               LL               ML   \n",
       "16                 ML                  LL               LL               ML   \n",
       "17                 ML                  LL               LL               ML   \n",
       "18                 ML                  LL               LL               ML   \n",
       "19                 ML                  LL               LL               ML   \n",
       "20                 ML                  LL               LL               ML   \n",
       "21                 ML                  LL               LL               ML   \n",
       "22                 ML                  LL               LL               ML   \n",
       "23                 ML                  LL               LL               ML   \n",
       "24                 ML                  LL               LL               ML   \n",
       "25                 HL                  ML               HL               LL   \n",
       "26                 HL                  ML               HL               LL   \n",
       "27                 HL                  ML               LL               LL   \n",
       "28                 HL                  ML               HL               LL   \n",
       "29                 HL                  ML               HL               LL   \n",
       "...               ...                 ...              ...              ...   \n",
       "2396               ML                  ML               ML               LL   \n",
       "2397               ML                  ML               ML               LL   \n",
       "2398               ML                  ML               ML               LL   \n",
       "2399               ML                  ML               ML               LL   \n",
       "2400               ML                  ML               ML               LL   \n",
       "2401               ML                  ML               LL               LL   \n",
       "2402               ML                  ML               ML               LL   \n",
       "2403               ML                  ML               ML               LL   \n",
       "2404               ML                  ML               ML               LL   \n",
       "2405               ML                  ML               ML               LL   \n",
       "2406               ML                  ML               ML               LL   \n",
       "2407               HL                  ML               LL               LL   \n",
       "2408               HL                  ML               LL               LL   \n",
       "2409               HL                  ML               LL               LL   \n",
       "2410               HL                  ML               LL               LL   \n",
       "2411               HL                  ML               LL               LL   \n",
       "2412               HL                  ML               LL               LL   \n",
       "2413               HL                  ML               LL               LL   \n",
       "2414               HL                  ML               LL               LL   \n",
       "2415               HL                  ML               LL               LL   \n",
       "2416               HL                  ML               LL               LL   \n",
       "2417               HL                  ML               LL               LL   \n",
       "2418               HL                  ML               LL               LL   \n",
       "2419               HL                  ML               LL               LL   \n",
       "2420               HL                  ML               LL               LL   \n",
       "2421               HL                  ML               LL               LL   \n",
       "2422               HL                  ML               LL               LL   \n",
       "2423               HL                  ML               LL               LL   \n",
       "2424               HL                  ML               LL               LL   \n",
       "2425               HL                  ML               LL               LL   \n",
       "\n",
       "     level_of_physics level_of_math learning_goal learning_style  profile_bias  \n",
       "0                  ML            ML           LLL            GLB      0.468446  \n",
       "1                  ML            ML           LLL            GLB     -0.798221  \n",
       "2                  ML            ML           LLL            GLB      1.001779  \n",
       "3                  ML            ML           LLL            GLB      0.912890  \n",
       "4                  ML            ML           LLL            GLB     -0.055364  \n",
       "5                  LL            ML           STL            GLB     -0.584840  \n",
       "6                  LL            ML           STL            GLB      0.243731  \n",
       "7                  LL            ML           STL            GLB     -0.351507  \n",
       "8                  LL            ML           STL            GLB     -0.284840  \n",
       "9                  LL            ML           STL            GLB     -0.740396  \n",
       "10                 LL            ML           STL            GLB     -0.584840  \n",
       "11                 LL            ML           STL            GLB     -0.059840  \n",
       "12                 LL            ML           STL            GLB      1.100874  \n",
       "13                 LL            ML           STL            GLB      0.731826  \n",
       "14                 LL            ML           STL            GLB      1.081826  \n",
       "15                 LL            ML           STL            GLB      1.087887  \n",
       "16                 LL            ML           STL            GLB      0.926271  \n",
       "17                 LL            ML           STL            GLB      0.315160  \n",
       "18                 LL            ML           STL            GLB      0.529445  \n",
       "19                 LL            ML           STL            GLB      0.815160  \n",
       "20                 LL            ML           STL            GLB     -1.184840  \n",
       "21                 LL            ML           STL            GLB     -0.384840  \n",
       "22                 LL            ML           STL            GLB     -1.327698  \n",
       "23                 LL            ML           STL            GLB      0.815160  \n",
       "24                 LL            ML           STL            GLB     -0.559840  \n",
       "25                 HL            HL           STL            GLB      0.135112  \n",
       "26                 HL            HL           STL            GLB      0.801779  \n",
       "27                 ML            HL           STL            GLB     -0.998221  \n",
       "28                 HL            HL           STL            GLB      1.256324  \n",
       "29                 HL            HL           STL            GLB     -0.198221  \n",
       "...               ...           ...           ...            ...           ...  \n",
       "2396               LL            ML           STL            SQN      1.148493  \n",
       "2397               LL            ML           STL            SQN      0.529445  \n",
       "2398               LL            ML           STL            SQN     -0.851507  \n",
       "2399               LL            ML           STL            SQN     -0.934840  \n",
       "2400               LL            ML           STL            SQN     -0.559840  \n",
       "2401               LL            ML           STL            SQN      1.100874  \n",
       "2402               LL            ML           STL            SQN      0.415160  \n",
       "2403               LL            ML           STL            SQN      0.981826  \n",
       "2404               LL            ML           STL            SQN      0.615160  \n",
       "2405               LL            ML           STL            SQN      0.565160  \n",
       "2406               LL            ML           STL            SQN     -0.584840  \n",
       "2407               ML            ML           LLL            SQN     -0.642666  \n",
       "2408               ML            ML           LLL            SQN      0.516065  \n",
       "2409               ML            ML           LLL            SQN      0.135112  \n",
       "2410               ML            ML           LLL            SQN      0.801779  \n",
       "2411               ML            ML           LLL            SQN      0.135112  \n",
       "2412               ML            ML           LLL            SQN      1.256324  \n",
       "2413               ML            ML           LLL            SQN      1.051779  \n",
       "2414               ML            ML           LLL            SQN      0.551779  \n",
       "2415               ML            ML           LLL            SQN     -1.073221  \n",
       "2416               ML            ML           LLL            SQN     -0.998221  \n",
       "2417               ML            ML           LLL            SQN      0.401779  \n",
       "2418               ML            ML           LLL            SQN      1.087493  \n",
       "2419               ML            ML           LLL            SQN     -0.055364  \n",
       "2420               ML            ML           LLL            SQN     -1.448221  \n",
       "2421               ML            ML           LLL            SQN      0.401779  \n",
       "2422               ML            ML           LLL            SQN     -0.698221  \n",
       "2423               ML            ML           LLL            SQN     -0.198221  \n",
       "2424               ML            ML           LLL            SQN     -0.698221  \n",
       "2425               ML            ML           LLL            SQN      0.801779  \n",
       "\n",
       "[2426 rows x 17 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1463821647187085"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the contextual factor bias CAMF-CCI (na verdade eu teria que fazer a influencia de cada \n",
    "# valor contextual sobre o item e nao de cada categoria de contexto sobre o item CAMF-CCI é uma adap de CAMF-CI)\n",
    "\n",
    "profile_bias = df.groupby(['learner_id'])['profile_bias'].mean()\n",
    "\n",
    "total_profile_bias = items_mean.mean()+profile_bias.mean()+items_bias.mean()\n",
    "\n",
    "total_profile_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df,test_size=0.25)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test matrix\n",
    "R = np.zeros((id_max, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    R[line[6]-1, line[5]-1] = line[2]  \n",
    "\n",
    "T = np.zeros((id_max, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    T[line[6]-1, line[5]-1] = line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = R.copy()\n",
    "I[I > 0] = 1\n",
    "I[I == 0] = 0\n",
    "\n",
    "# Index matrix for test data\n",
    "I2 = T.copy()\n",
    "I2[I2 > 0] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the unknown ratings through the dot product of the latent features for users and items \n",
    "def prediction(P,Q,IM,UB,IB):\n",
    "    return (np.dot(P.T,Q)+IM+UB+IB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmbda = 0.5 # Regularisation weight\n",
    "k = 8  # Dimensionality of the latent feature space\n",
    "m, n = R.shape  # Number of users and items\n",
    "n_epochs = 100  # Number of epochs\n",
    "gamma=0.005  # Learning rate\n",
    "\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "def rmse(I,R,Q,P):\n",
    "    return np.sqrt(np.sum((I * (R - (np.dot(P.T,Q)+total_profile_bias))**2)/len(R[R > 0])))  ##alterar o calculo do RMSE ta errado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Train error:3.25536574985-->Test error:3.7205271064\n",
      "Epoch[2/100] Train error:2.22156190906-->Test error:2.71132997975\n",
      "Epoch[3/100] Train error:1.89308043836-->Test error:2.35291508884\n",
      "Epoch[4/100] Train error:1.73557657095-->Test error:2.16208169415\n",
      "Epoch[5/100] Train error:1.64529949789-->Test error:2.04048054482\n",
      "Epoch[6/100] Train error:1.58869429832-->Test error:1.95533841012\n",
      "Epoch[7/100] Train error:1.55111310999-->Test error:1.8921039343\n",
      "Epoch[8/100] Train error:1.52508816326-->Test error:1.84316692517\n",
      "Epoch[9/100] Train error:1.50644540223-->Test error:1.80411128439\n",
      "Epoch[10/100] Train error:1.49270030135-->Test error:1.77218154574\n",
      "Epoch[11/100] Train error:1.48230340086-->Test error:1.74556347759\n",
      "Epoch[12/100] Train error:1.47425277121-->Test error:1.72301193423\n",
      "Epoch[13/100] Train error:1.46788145278-->Test error:1.70364353035\n",
      "Epoch[14/100] Train error:1.46273472115-->Test error:1.68681423312\n",
      "Epoch[15/100] Train error:1.45849622702-->Test error:1.67204364259\n",
      "Epoch[16/100] Train error:1.45494201693-->Test error:1.65896632771\n",
      "Epoch[17/100] Train error:1.45191109283-->Test error:1.64729953585\n",
      "Epoch[18/100] Train error:1.44928610871-->Test error:1.63682117731\n",
      "Epoch[19/100] Train error:1.44698045452-->Test error:1.62735446072\n",
      "Epoch[20/100] Train error:1.44492946059-->Test error:1.61875694939\n",
      "Epoch[21/100] Train error:1.44308431465-->Test error:1.61091262562\n",
      "Epoch[22/100] Train error:1.44140779659-->Test error:1.60372604383\n",
      "Epoch[23/100] Train error:1.43987125022-->Test error:1.5971179607\n",
      "Epoch[24/100] Train error:1.43845240877-->Test error:1.5910220268\n",
      "Epoch[25/100] Train error:1.43713381698-->Test error:1.58538225247\n",
      "Epoch[26/100] Train error:1.4359016749-->Test error:1.58015104559\n",
      "Epoch[27/100] Train error:1.43474498279-->Test error:1.57528767734\n",
      "Epoch[28/100] Train error:1.43365490349-->Test error:1.57075707148\n",
      "Epoch[29/100] Train error:1.43262428284-->Test error:1.56652884083\n",
      "Epoch[30/100] Train error:1.43164728625-->Test error:1.56257651442\n",
      "Epoch[31/100] Train error:1.43071912133-->Test error:1.55887691297\n",
      "Epoch[32/100] Train error:1.42983582461-->Test error:1.55540964072\n",
      "Epoch[33/100] Train error:1.42899409646-->Test error:1.5521566692\n",
      "Epoch[34/100] Train error:1.42819117254-->Test error:1.54910199431\n",
      "Epoch[35/100] Train error:1.42742472302-->Test error:1.54623135195\n",
      "Epoch[36/100] Train error:1.42669277312-->Test error:1.54353198108\n",
      "Epoch[37/100] Train error:1.42599364011-->Test error:1.54099242515\n",
      "Epoch[38/100] Train error:1.42532588311-->Test error:1.53860236485\n",
      "Epoch[39/100] Train error:1.42468826276-->Test error:1.53635247664\n",
      "Epoch[40/100] Train error:1.42407970881-->Test error:1.5342343124\n",
      "Epoch[41/100] Train error:1.42349929383-->Test error:1.53224019672\n",
      "Epoch[42/100] Train error:1.42294621179-->Test error:1.53036313878\n",
      "Epoch[43/100] Train error:1.42241976061-->Test error:1.52859675649\n",
      "Epoch[44/100] Train error:1.42191932771-->Test error:1.52693521084\n",
      "Epoch[45/100] Train error:1.42144437818-->Test error:1.52537314904\n",
      "Epoch[46/100] Train error:1.42099444486-->Test error:1.52390565484\n",
      "Epoch[47/100] Train error:1.42056912017-->Test error:1.52252820518\n",
      "Epoch[48/100] Train error:1.42016804913-->Test error:1.52123663213\n",
      "Epoch[49/100] Train error:1.4197909236-->Test error:1.52002708937\n",
      "Epoch[50/100] Train error:1.41943747731-->Test error:1.51889602255\n",
      "Epoch[51/100] Train error:1.41910748164-->Test error:1.51784014309\n",
      "Epoch[52/100] Train error:1.41880074206-->Test error:1.51685640479\n",
      "Epoch[53/100] Train error:1.41851709492-->Test error:1.51594198301\n",
      "Epoch[54/100] Train error:1.41825640483-->Test error:1.51509425607\n",
      "Epoch[55/100] Train error:1.41801856221-->Test error:1.5143107885\n",
      "Epoch[56/100] Train error:1.41780348123-->Test error:1.51358931596\n",
      "Epoch[57/100] Train error:1.41761109792-->Test error:1.5129277317\n",
      "Epoch[58/100] Train error:1.41744136849-->Test error:1.51232407421\n",
      "Epoch[59/100] Train error:1.41729426783-->Test error:1.51177651609\n",
      "Epoch[60/100] Train error:1.41716978809-->Test error:1.51128335382\n",
      "Epoch[61/100] Train error:1.41706793737-->Test error:1.51084299851\n",
      "Epoch[62/100] Train error:1.41698873858-->Test error:1.5104539674\n",
      "Epoch[63/100] Train error:1.41693222825-->Test error:1.510114876\n",
      "Epoch[64/100] Train error:1.41689845545-->Test error:1.50982443095\n",
      "Epoch[65/100] Train error:1.41688748076-->Test error:1.50958142337\n",
      "Epoch[66/100] Train error:1.41689937526-->Test error:1.50938472266\n",
      "Epoch[67/100] Train error:1.41693421954-->Test error:1.50923327088\n",
      "Epoch[68/100] Train error:1.41699210267-->Test error:1.50912607734\n",
      "Epoch[69/100] Train error:1.41707312131-->Test error:1.50906221371\n",
      "Epoch[70/100] Train error:1.41717737866-->Test error:1.5090408093\n",
      "Epoch[71/100] Train error:1.41730498351-->Test error:1.50906104674\n",
      "Epoch[72/100] Train error:1.41745604923-->Test error:1.50912215783\n",
      "Epoch[73/100] Train error:1.41763069277-->Test error:1.50922341968\n",
      "Epoch[74/100] Train error:1.41782903365-->Test error:1.50936415099\n",
      "Epoch[75/100] Train error:1.41805119286-->Test error:1.50954370854\n",
      "Epoch[76/100] Train error:1.41829729182-->Test error:1.5097614839\n",
      "Epoch[77/100] Train error:1.41856745129-->Test error:1.51001690016\n",
      "Epoch[78/100] Train error:1.41886179022-->Test error:1.51030940895\n",
      "Epoch[79/100] Train error:1.41918042461-->Test error:1.51063848744\n",
      "Epoch[80/100] Train error:1.41952346631-->Test error:1.51100363555\n",
      "Epoch[81/100] Train error:1.41989102187-->Test error:1.51140437321\n",
      "Epoch[82/100] Train error:1.42028319123-->Test error:1.51184023772\n",
      "Epoch[83/100] Train error:1.42070006654-->Test error:1.51231078122\n",
      "Epoch[84/100] Train error:1.42114173082-->Test error:1.51281556821\n",
      "Epoch[85/100] Train error:1.42160825671-->Test error:1.51335417317\n",
      "Epoch[86/100] Train error:1.42209970516-->Test error:1.51392617822\n",
      "Epoch[87/100] Train error:1.42261612408-->Test error:1.51453117091\n",
      "Epoch[88/100] Train error:1.42315754704-->Test error:1.51516874202\n",
      "Epoch[89/100] Train error:1.42372399193-->Test error:1.51583848347\n",
      "Epoch[90/100] Train error:1.42431545967-->Test error:1.51653998625\n",
      "Epoch[91/100] Train error:1.42493193285-->Test error:1.51727283854\n",
      "Epoch[92/100] Train error:1.42557337449-->Test error:1.5180366237\n",
      "Epoch[93/100] Train error:1.42623972676-->Test error:1.51883091861\n",
      "Epoch[94/100] Train error:1.42693090975-->Test error:1.51965529181\n",
      "Epoch[95/100] Train error:1.42764682027-->Test error:1.52050930195\n",
      "Epoch[96/100] Train error:1.42838733076-->Test error:1.52139249623\n",
      "Epoch[97/100] Train error:1.42915228818-->Test error:1.52230440893\n",
      "Epoch[98/100] Train error:1.42994151301-->Test error:1.52324456006\n",
      "Epoch[99/100] Train error:1.43075479836-->Test error:1.5242124542\n",
      "Epoch[100/100] Train error:1.43159190909-->Test error:1.52520757928\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "#Only consider non-zero matrix \n",
    "users,items = R.nonzero()      \n",
    "for epoch in range(n_epochs):\n",
    "    for u, i in zip(users,items):\n",
    "        e = R[u, i] - prediction(P[:,u],Q[:,i],items_mean[i+1],profile_bias[u+1],items_bias[i+1])  # Calculate error for gradient\n",
    "        P[:,u] += gamma * ( e * Q[:,i] - lmbda * P[:,u]) # Update latent user feature matrix\n",
    "        Q[:,i] += gamma * ( e * P[:,u] - lmbda * Q[:,i])  # Update latent movie feature matrix\n",
    "    #print('user:'+ str(u) + '----item:' + str(i) + '----rating:' + str(R[u, i]) + '----pred:' + str(prediction(P[:,u],Q[:,i],items_mean[i+1],users_bias[u+1],items_bias[i+1])))\n",
    "    train_rmse = rmse(I,R,Q,P) # Calculate root mean squared error from train dataset\n",
    "    test_rmse = rmse(I2,T,Q,P) # Calculate root mean squared error from test dataset\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    print(\"Epoch[\"+ str(epoch+1) + \"/\" + str(n_epochs) + \"] Train error:\" + str(train_rmse) + \"-->Test error:\" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "plt.title('SGD-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to perform quite well, with a relatively low RMSE after convergence. The performance can be influenced by tweaking the hyperparameters $\\lambda$, $\\gamma$ and $k$. In order to learn more about hyperparameter tuning you can take a look at one of the previous [posts](http://online.cambridgecoding.com/notebooks/cca_admin/scanning-hyperspace-how-to-tune-machine-learning-models). \n",
    "\n",
    "Next you could compare the actual rating with the predicted rating. To do this you first calculate the prediction matrix – for that you can use ``prediction`` function you have implemented above and convert it to a dataframe for the ease of use.<img src=\"https://latex.codecogs.com/gif.latex?\\hat&space;r_{ui}=P_u^TQ_i$&space;&space;$(2)\" title=\"\\hat r_{ui}=p_u^Tq_i\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate prediction matrix R_hat (low-rank approximation for R)\n",
    "R = pd.DataFrame(R)\n",
    "R_hat=pd.DataFrame(prediction(P,Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what you achieved, let's compare some of our predictions for user ``17`` with their actual ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare true ratings of user 17 with predictions\n",
    "ratings = pd.DataFrame(data=R.loc[16,R.loc[16,:] > 0]).head(n=5)\n",
    "ratings['Prediction'] = R_hat.loc[16,R.loc[16,:] > 0]\n",
    "ratings.columns = ['Actual Rating', 'Predicted Rating']\n",
    "ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
