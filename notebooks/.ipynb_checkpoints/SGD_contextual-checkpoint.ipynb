{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 78 | Number of movies = 405\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/rating_explicit_only.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "n_users = df.learner_id.unique().shape[0]\n",
    "n_items = df.movie_id.unique().shape[0]\n",
    "\n",
    "print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum learner_id in this list is= 111\n"
     ]
    }
   ],
   "source": [
    "id_max = df.sort_values(by=['learner_id'], ascending=False).head(n=1)\n",
    "\n",
    "id_max = int(id_max.learner_id)\n",
    "\n",
    "print('The maximum learner_id in this list is= ' + str(id_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = pd.read_csv('../data/movie.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "df['movie_knowledge_area'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn = pd.read_csv('../data/learner.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "lrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#copy the value of knowledge area from movies.csv\n",
    "for index, row in df.iterrows():\n",
    "    target_mv = mv.loc[mv.id == row.movie_id]\n",
    "    df.movie_knowledge_area.iloc[index] = target_mv.movie_knowledge_area.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_knowledge_area\n",
       "BL    3.457831\n",
       "EN    3.674419\n",
       "HT    3.426752\n",
       "LT    3.282427\n",
       "MT    3.600000\n",
       "PH    3.525510\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rating mean grouped by movie knowledge area\n",
    "\n",
    "mean_ratings = df.groupby(['movie_knowledge_area'])['rating'].mean()\n",
    "\n",
    "mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2426, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the profile features to the ratings dataframe\n",
    "\n",
    "df['level_of_education'], df['level_of_english'], df['level_of_literature'], df['level_of_history'], df['level_of_biology'], df['level_of_physics'], df['level_of_math'], df['learning_goal'], df['learning_style'] = ['', '', '', '', '', '', '', '', ''] \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#copy the profile levels from learner.csv\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    target_lrn = lrn.loc[lrn.id == row.learner_id]\n",
    "    df.level_of_education.iloc[index] = target_lrn.level_of_education.values[0]\n",
    "    df.level_of_english.iloc[index] = target_lrn.level_of_english.values[0]\n",
    "    df.level_of_literature.iloc[index] = target_lrn.level_of_literature.values[0]\n",
    "    df.level_of_history.iloc[index] = target_lrn.level_of_history.values[0]\n",
    "    df.level_of_biology.iloc[index] = target_lrn.level_of_biology.values[0]\n",
    "    df.level_of_physics.iloc[index] = target_lrn.level_of_physics.values[0]\n",
    "    df.level_of_math.iloc[index] = target_lrn.level_of_math.values[0]\n",
    "    df.learning_goal.iloc[index] = target_lrn.learning_goal.values[0]\n",
    "    df.learning_style.iloc[index] = target_lrn.learning_style.values[0]\n",
    "#     print(target_lrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rating mean grouped by levels\n",
    "\n",
    "mean_education = df.groupby(['level_of_education'])['rating'].mean()\n",
    "mean_english = df.groupby(['level_of_english'])['rating'].mean()\n",
    "mean_literature = df.groupby(['level_of_literature'])['rating'].mean()\n",
    "mean_history = df.groupby(['level_of_history'])['rating'].mean()\n",
    "mean_biology = df.groupby(['level_of_biology'])['rating'].mean()\n",
    "mean_physics = df.groupby(['level_of_physics'])['rating'].mean()\n",
    "mean_math = df.groupby(['level_of_math'])['rating'].mean()\n",
    "mean_lgoal = df.groupby(['learning_goal'])['rating'].mean()\n",
    "mean_lstyle = df.groupby(['learning_style'])['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "level_of_education\n",
      "DT    3.135674\n",
      "EF    5.000000\n",
      "EM    3.790541\n",
      "ES    3.008292\n",
      "MT    3.394343\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_english\n",
      "HL    3.198221\n",
      "LL    3.910000\n",
      "ML    3.184840\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_literature\n",
      "HL    3.456967\n",
      "LL    2.880282\n",
      "ML    3.282482\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_history\n",
      "HL    3.702875\n",
      "LL    3.261628\n",
      "ML    3.117095\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_biology\n",
      "HL    3.940120\n",
      "LL    3.231368\n",
      "ML    3.130147\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_physics\n",
      "HL    3.236318\n",
      "LL    3.351049\n",
      "ML    3.149733\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "level_of_math\n",
      "HL    3.002795\n",
      "LL    3.743976\n",
      "ML    3.438914\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "learning_goal\n",
      "LLL    3.186104\n",
      "STL    3.260477\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "learning_style\n",
      "GLB    3.329248\n",
      "SQN    3.115641\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Means:\\n' + str(mean_education) + '\\n\\n' + str(mean_english) + '\\n\\n' + str(mean_literature) + '\\n\\n' + str(mean_history) + '\\n\\n' + str(mean_biology) + '\\n\\n' + str(mean_physics) + '\\n\\n' + str(mean_math)+ '\\n\\n' + str(mean_lgoal)+ '\\n\\n' + str(mean_lstyle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/ratings_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.071537344685492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the item's rating mean\n",
    "\n",
    "# df.rating = pd.to_numeric(df.rating, downcast = 'integer', errors='coerce')\n",
    "\n",
    "items_mean = df.groupby(['movie_id'])['rating'].mean()\n",
    "overall_mean = items_mean.mean()\n",
    "\n",
    "overall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.3642514867149e-16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the item bias\n",
    "\n",
    "items_bias = items_mean - overall_mean\n",
    "\n",
    "overall_bias = items_bias.mean()\n",
    "\n",
    "overall_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #get specific user bias (baseline)\n",
    "\n",
    "# def user_bias(user):\n",
    "#     u_index = users_mean.index.get_loc(user) #get integer index through the label index\n",
    "#     u_mean = users_mean[u_index] #get user mean\n",
    "#     u_bias = u_mean - overall_mean #compute user bias\n",
    "#     return(u_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# save the bias on dataframe\n",
    "df['profile_bias'] = np.nan\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row.level_of_literature == 'HL':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_literature[0])\n",
    "    elif row.level_of_literature == 'LL':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_literature[1])\n",
    "    elif row.level_of_literature == 'ML':\n",
    "        df.profile_bias.iloc[index] = float(items_mean[row.movie_id]) - float(mean_literature[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/ratings_bias.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1771793656197964"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the contextual factor bias CAMF-CCI (na verdade eu teria que fazer a influencia de cada \n",
    "# valor contextual sobre o item e nao de cada categoria de contexto sobre o item CAMF-CCI é uma adap de CAMF-CI)\n",
    "\n",
    "profile_bias = df.groupby(['learner_id'])['profile_bias'].mean()\n",
    "\n",
    "total_profile_bias = items_mean.mean()+profile_bias.mean()+items_bias.mean()\n",
    "\n",
    "total_profile_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df,test_size=0.25)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the specific biases\n",
    "def biases(B):\n",
    "    #items mean\n",
    "    train_mean = train_data.groupby(['movie_id'])['rating'].mean()\n",
    "    test_mean = test_data.groupby(['movie_id'])['rating'].mean()\n",
    "\n",
    "    #profile bias\n",
    "    train_pbias = train_data.groupby(['learner_id'])['profile_bias'].mean()\n",
    "    test_pbias = test_data.groupby(['learner_id'])['profile_bias'].mean()\n",
    "\n",
    "    #items bias\n",
    "    train_bias = train_mean - train_mean.mean()\n",
    "    test_bias = test_mean = test_mean.mean()\n",
    "\n",
    "    if B =='train':\n",
    "        return train_mean.mean()+train_pbias.mean()+train_bias.mean()\n",
    "    elif B =='test':\n",
    "        return test_mean.mean()+test_pbias.mean()+test_bias.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test matrix\n",
    "R = np.zeros((id_max, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    R[line[6]-1, line[5]-1] = line[2]  \n",
    "\n",
    "T = np.zeros((id_max, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    T[line[6]-1, line[5]-1] = line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = R.copy()\n",
    "I[I > 0] = 1\n",
    "I[I == 0] = 0\n",
    "\n",
    "# Index matrix for test data\n",
    "I2 = T.copy()\n",
    "I2[I2 > 0] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the unknown ratings through the dot product of the latent features for users and items \n",
    "def prediction(P,Q,IM,UB,IB):\n",
    "    return (np.dot(P.T,Q)+IM+UB+IB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmbda = 0.1 # Regularisation weight\n",
    "k = 20  # Dimensionality of the latent feature space\n",
    "m, n = R.shape  # Number of users and items\n",
    "n_epochs = 100  # Number of epochs\n",
    "gamma=0.001  # Learning rate\n",
    "\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "def rmse(I,R,Q,P,B):\n",
    "    return np.sqrt(np.sum((I * (R - (np.dot(P.T,Q)+bias(B)))**2)/len(R[R > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate MAE\n",
    "def mae(I,R,Q,P,B):\n",
    "    return np.sum(abs((I * (R - (np.dot(P.T,Q)+bias(B)))/len(R[R > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Train error:10.3570343648-->Test error:11.0804431117\n",
      "Epoch[2/100] Train error:5.62013051851-->Test error:6.47099575676\n",
      "Epoch[3/100] Train error:3.97849594748-->Test error:4.81730019215\n",
      "Epoch[4/100] Train error:3.18599658685-->Test error:3.99466785396\n",
      "Epoch[5/100] Train error:2.76402558497-->Test error:3.52437761622\n",
      "Epoch[6/100] Train error:2.50590691142-->Test error:3.24329148307\n",
      "Epoch[7/100] Train error:2.32847509946-->Test error:3.06826016294\n",
      "Epoch[8/100] Train error:2.19696896753-->Test error:2.94457446762\n",
      "Epoch[9/100] Train error:2.09335470432-->Test error:2.85283190658\n",
      "Epoch[10/100] Train error:2.00864266439-->Test error:2.77970236774\n",
      "Epoch[11/100] Train error:1.93734614682-->Test error:2.71924701146\n",
      "Epoch[12/100] Train error:1.87718875966-->Test error:2.66813258895\n",
      "Epoch[13/100] Train error:1.82476433315-->Test error:2.62429248006\n",
      "Epoch[14/100] Train error:1.77844257649-->Test error:2.58700559179\n",
      "Epoch[15/100] Train error:1.73752037429-->Test error:2.55400144819\n",
      "Epoch[16/100] Train error:1.70128766776-->Test error:2.52456048034\n",
      "Epoch[17/100] Train error:1.67010053918-->Test error:2.49823073136\n",
      "Epoch[18/100] Train error:1.64278498027-->Test error:2.47430996038\n",
      "Epoch[19/100] Train error:1.61799507712-->Test error:2.45240580723\n",
      "Epoch[20/100] Train error:1.59552031814-->Test error:2.43211457956\n",
      "Epoch[21/100] Train error:1.57471039754-->Test error:2.41334861277\n",
      "Epoch[22/100] Train error:1.55585815484-->Test error:2.39571009371\n",
      "Epoch[23/100] Train error:1.5390596041-->Test error:2.37921419824\n",
      "Epoch[24/100] Train error:1.52379200638-->Test error:2.36393907882\n",
      "Epoch[25/100] Train error:1.50939099196-->Test error:2.34967400478\n",
      "Epoch[26/100] Train error:1.4961166181-->Test error:2.3362581854\n",
      "Epoch[27/100] Train error:1.4839025467-->Test error:2.32355752628\n",
      "Epoch[28/100] Train error:1.47256884118-->Test error:2.31176272857\n",
      "Epoch[29/100] Train error:1.46192508547-->Test error:2.30094708461\n",
      "Epoch[30/100] Train error:1.45206958862-->Test error:2.29094398284\n",
      "Epoch[31/100] Train error:1.44277314886-->Test error:2.28142295189\n",
      "Epoch[32/100] Train error:1.4343443287-->Test error:2.27237859728\n",
      "Epoch[33/100] Train error:1.42644245045-->Test error:2.26382330258\n",
      "Epoch[34/100] Train error:1.41909285029-->Test error:2.25561672302\n",
      "Epoch[35/100] Train error:1.41227707749-->Test error:2.24769462807\n",
      "Epoch[36/100] Train error:1.40578767356-->Test error:2.24017644517\n",
      "Epoch[37/100] Train error:1.39971723818-->Test error:2.23308348069\n",
      "Epoch[38/100] Train error:1.39403446913-->Test error:2.2262491637\n",
      "Epoch[39/100] Train error:1.38869028829-->Test error:2.21966991031\n",
      "Epoch[40/100] Train error:1.38374051029-->Test error:2.21335903714\n",
      "Epoch[41/100] Train error:1.37911256013-->Test error:2.20741624331\n",
      "Epoch[42/100] Train error:1.3748733328-->Test error:2.20168483157\n",
      "Epoch[43/100] Train error:1.37089792874-->Test error:2.19614008478\n",
      "Epoch[44/100] Train error:1.36713739904-->Test error:2.19073634246\n",
      "Epoch[45/100] Train error:1.36374411267-->Test error:2.18554843024\n",
      "Epoch[46/100] Train error:1.36060996501-->Test error:2.18049517566\n",
      "Epoch[47/100] Train error:1.35765992434-->Test error:2.17557137222\n",
      "Epoch[48/100] Train error:1.35486110493-->Test error:2.17089503787\n",
      "Epoch[49/100] Train error:1.3522893169-->Test error:2.1663464958\n",
      "Epoch[50/100] Train error:1.34983217139-->Test error:2.16189386296\n",
      "Epoch[51/100] Train error:1.34748534005-->Test error:2.15760066934\n",
      "Epoch[52/100] Train error:1.34528254949-->Test error:2.15348850325\n",
      "Epoch[53/100] Train error:1.3431750313-->Test error:2.14949047987\n",
      "Epoch[54/100] Train error:1.34117327792-->Test error:2.14558791236\n",
      "Epoch[55/100] Train error:1.3393011981-->Test error:2.14180350934\n",
      "Epoch[56/100] Train error:1.33756381458-->Test error:2.13816145953\n",
      "Epoch[57/100] Train error:1.33594693907-->Test error:2.13458469985\n",
      "Epoch[58/100] Train error:1.33440580868-->Test error:2.13107075701\n",
      "Epoch[59/100] Train error:1.33296670079-->Test error:2.12761730293\n",
      "Epoch[60/100] Train error:1.33159930816-->Test error:2.12423543298\n",
      "Epoch[61/100] Train error:1.33030583012-->Test error:2.12096599678\n",
      "Epoch[62/100] Train error:1.32907758501-->Test error:2.11776075544\n",
      "Epoch[63/100] Train error:1.3279107682-->Test error:2.1146245179\n",
      "Epoch[64/100] Train error:1.32678982796-->Test error:2.11157590172\n",
      "Epoch[65/100] Train error:1.32572887184-->Test error:2.10857379716\n",
      "Epoch[66/100] Train error:1.32472086716-->Test error:2.10561661157\n",
      "Epoch[67/100] Train error:1.32374651552-->Test error:2.10270291259\n",
      "Epoch[68/100] Train error:1.32283642462-->Test error:2.0998313398\n",
      "Epoch[69/100] Train error:1.32196347579-->Test error:2.09700059981\n",
      "Epoch[70/100] Train error:1.32111806693-->Test error:2.09422812123\n",
      "Epoch[71/100] Train error:1.32033967747-->Test error:2.09152396284\n",
      "Epoch[72/100] Train error:1.31963446258-->Test error:2.08888959156\n",
      "Epoch[73/100] Train error:1.31896966628-->Test error:2.08628947376\n",
      "Epoch[74/100] Train error:1.31837693417-->Test error:2.08372265865\n",
      "Epoch[75/100] Train error:1.31782676149-->Test error:2.08118823754\n",
      "Epoch[76/100] Train error:1.31731975948-->Test error:2.07870462204\n",
      "Epoch[77/100] Train error:1.31684309217-->Test error:2.07626233611\n",
      "Epoch[78/100] Train error:1.316391674-->Test error:2.07384950155\n",
      "Epoch[79/100] Train error:1.31596961512-->Test error:2.07149893079\n",
      "Epoch[80/100] Train error:1.31558636053-->Test error:2.06919406221\n",
      "Epoch[81/100] Train error:1.31523035658-->Test error:2.06691547957\n",
      "Epoch[82/100] Train error:1.31490858784-->Test error:2.06466253918\n",
      "Epoch[83/100] Train error:1.31464200545-->Test error:2.06243462232\n",
      "Epoch[84/100] Train error:1.31441650714-->Test error:2.06023113384\n",
      "Epoch[85/100] Train error:1.31420800244-->Test error:2.05805150091\n",
      "Epoch[86/100] Train error:1.31401663735-->Test error:2.05591353294\n",
      "Epoch[87/100] Train error:1.31384734835-->Test error:2.05380664931\n",
      "Epoch[88/100] Train error:1.31370164211-->Test error:2.05174900217\n",
      "Epoch[89/100] Train error:1.31355893934-->Test error:2.04976582496\n",
      "Epoch[90/100] Train error:1.31342398397-->Test error:2.04780237003\n",
      "Epoch[91/100] Train error:1.31330636055-->Test error:2.04592471594\n",
      "Epoch[92/100] Train error:1.31319338406-->Test error:2.04406526539\n",
      "Epoch[93/100] Train error:1.31308932626-->Test error:2.04222323979\n",
      "Epoch[94/100] Train error:1.31299849039-->Test error:2.0403982739\n",
      "Epoch[95/100] Train error:1.31291354197-->Test error:2.03859716181\n",
      "Epoch[96/100] Train error:1.31284432378-->Test error:2.03682147021\n",
      "Epoch[97/100] Train error:1.31277950883-->Test error:2.03506148141\n",
      "Epoch[98/100] Train error:1.31272885227-->Test error:2.03333118706\n",
      "Epoch[99/100] Train error:1.31267991833-->Test error:2.03162912734\n",
      "Epoch[100/100] Train error:1.31263801166-->Test error:2.02994144547\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "#Only consider non-zero matrix \n",
    "users,items = R.nonzero()      \n",
    "for epoch in range(n_epochs):\n",
    "    error = 0\n",
    "    for u, i in zip(users,items):\n",
    "        e = R[u, i] - prediction(P[:,u],Q[:,i],items_mean[i+1],profile_bias[u+1],items_bias[i+1])  # Calculate error for gradient\n",
    "        P[:,u] += gamma * ( e * Q[:,i] - lmbda * P[:,u]) # Update latent user feature matrix\n",
    "        Q[:,i] += gamma * ( e * P[:,u] - lmbda * Q[:,i])  # Update latent movie feature matrix\n",
    "        error += e\n",
    "#     print(abs(error/users.shape[0]))\n",
    "    train_rmse = rmse(I,R,Q,P,'train') # Calculate root mean squared error from train dataset\n",
    "    test_rmse = rmse(I2,T,Q,P,'test') # Calculate root mean squared error from test dataset\n",
    "    train_mae = mae(I,R,Q,P,'train')\n",
    "    test_mae = mae(I2,T,Q,P,'test')\n",
    "    train_errors.append(train_mae)\n",
    "    test_errors.append(test_mae)\n",
    "    print(\"Epoch[\"+ str(epoch+1) + \"/\" + str(n_epochs) + \"] Train error:\" + str(train_mae) + \"-->Test error:\" + str(test_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "plt.title('SGD-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to perform quite well, with a relatively low RMSE after convergence. The performance can be influenced by tweaking the hyperparameters $\\lambda$, $\\gamma$ and $k$. In order to learn more about hyperparameter tuning you can take a look at one of the previous [posts](http://online.cambridgecoding.com/notebooks/cca_admin/scanning-hyperspace-how-to-tune-machine-learning-models). \n",
    "\n",
    "Next you could compare the actual rating with the predicted rating. To do this you first calculate the prediction matrix – for that you can use ``prediction`` function you have implemented above and convert it to a dataframe for the ease of use.<img src=\"https://latex.codecogs.com/gif.latex?\\hat&space;r_{ui}=P_u^TQ_i$&space;&space;$(2)\" title=\"\\hat r_{ui}=p_u^Tq_i\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate prediction matrix R_hat (low-rank approximation for R)\n",
    "R = pd.DataFrame(R)\n",
    "R_hat=pd.DataFrame(prediction(P,Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what you achieved, let's compare some of our predictions for user ``17`` with their actual ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare true ratings of user 17 with predictions\n",
    "ratings = pd.DataFrame(data=R.loc[16,R.loc[16,:] > 0]).head(n=5)\n",
    "ratings['Prediction'] = R_hat.loc[16,R.loc[16,:] > 0]\n",
    "ratings.columns = ['Actual Rating', 'Predicted Rating']\n",
    "ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
